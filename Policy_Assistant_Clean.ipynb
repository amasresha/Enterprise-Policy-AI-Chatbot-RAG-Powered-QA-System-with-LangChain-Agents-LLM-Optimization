{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93286be3",
   "metadata": {
    "id": "1735c566-302d-48ad-88f7-7b52ee1ae4b3"
   },
   "source": [
    "# **Enterprise Policy AI Chatbot: RAG-Powered QA System with LangChain Agents & LLM Optimization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a59cd0",
   "metadata": {
    "id": "362bb42d-92b1-4b2b-8e48-9231c14c969e"
   },
   "source": [
    "## **Overview**  \n",
    "In this project, we will build an AI-powered assistant that can read and summarize private documents using **Retrieval-Augmented Generation (RAG)**, **LangChain**, and **LLMs**. This enables quick access to essential information without manually reading large volumes of text.\n",
    "\n",
    "## **Steps to Build the Solution**  \n",
    "\n",
    "### **1. Understanding RAG & LangChain**  \n",
    "- **RAG (Retrieval-Augmented Generation)**: Enhances LLMs by incorporating external/private data for better responses.  \n",
    "- **LangChain**: A framework for building applications that leverage LLMs with external data retrieval.\n",
    "\n",
    "### **2. Setting Up the Environment**  \n",
    "- Install required Python libraries: `langchain`, `faiss`, `openai`, etc.  \n",
    "- Import necessary modules for document processing.\n",
    "\n",
    "### **3. Preprocessing Documents**  \n",
    "- **Load the document**: Use `DocumentLoaders` to read files.  \n",
    "- **Split into chunks**: Break large text into manageable parts.  \n",
    "- **Embed and store**: Convert text into embeddings and store in a vector database like **FAISS**.\n",
    "\n",
    "### **4. Implementing the RAG Pipeline**  \n",
    "- **Indexing**: Prepare and store documents for retrieval.  \n",
    "- **Retrieval & Generation**:  \n",
    "  - Retrieve relevant sections from the vector store based on user queries.  \n",
    "  - Use an LLM to generate responses using retrieved information.\n",
    "\n",
    "### **5. Enhancing with LangChain Features**  \n",
    "- **Prompt Templates**: Structure inputs for better results.  \n",
    "- **Memory**: Make conversations context-aware.  \n",
    "- **Agent Integration**: Automate summarization with AI agents.\n",
    "\n",
    "\n",
    "## **Key Takeaways**  \n",
    "By the end of this project, you will have a fully functional **document summarization assistant** that allows you to efficiently extract insights from private files without exposing them to external AI services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639cdaf",
   "metadata": {
    "id": "WccBq10fzwA2"
   },
   "source": [
    "\n",
    "### RAG Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440a4e4",
   "metadata": {
    "id": "tGZGaj4gu-dt"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "image_path = \"RAG_Architecture.PNG\"\n",
    "\n",
    "Image(filename=image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1896e8",
   "metadata": {
    "id": "785bb7b6-0326-4c07-a97b-37767df75f99"
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aac443",
   "metadata": {
    "id": "Qtvro-UAZHdq"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --user \"langchain==0.1.16\"\n",
    "!pip install --user \"huggingface == 0.0.1\"\n",
    "!pip install --user \"huggingface-hub == 0.23.4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2fd8c2",
   "metadata": {
    "id": "8c64f36e-d0e2-4356-a2bb-98454f9ef199"
   },
   "source": [
    "After the installation of libraries is completed, restart your kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda0ece3",
   "metadata": {
    "id": "3d5ef7ec-3022-4b8b-8b03-abca7ab287bc"
   },
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fbe1b7",
   "metadata": {
    "id": "dqRimzG0SR_W"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# 1. Clean previous installations\n",
    "!pip uninstall -y torch torchvision torchaudio sentence-transformers transformers\n",
    "\n",
    "# 2. Install CPU-only PyTorch + torchvision (compatible versions)\n",
    "!pip install torch==2.1.0+cpu torchvision==0.16.0+cpu torchaudio==0.13.0 --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# 3. Install sentence-transformers (specific version that works with PyTorch 2.1.0)\n",
    "!pip install sentence-transformers==2.5.1\n",
    "\n",
    "# 4.  Ensure HuggingFace transformers is installed\n",
    "!pip install transformers==4.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd6e41",
   "metadata": {
    "id": "MZqc8tfnTNpl"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# 1. Clean slate (ignore warnings)\n",
    "!pip uninstall -y torch torchvision torchaudio sentence-transformers transformers timm fastai\n",
    "\n",
    "# 2. Install CPU-only versions (critical: use --no-deps to prevent auto-upgrades)\n",
    "!pip install --no-cache-dir --no-deps torch==2.1.0+cpu torchvision==0.16.0+cpu --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# 3. Install sentence-transformers with pinned dependencies\n",
    "!pip install --no-cache-dir sentence-transformers==2.5.1 transformers==4.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c72e1",
   "metadata": {
    "id": "jRodHIYeTcdF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sentence-transformers\n",
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4597e9a",
   "metadata": {
    "id": "22k2llgnyjiZ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d06aab",
   "metadata": {
    "id": "bc636389-b116-4219-bb30-8e07e21015da"
   },
   "outputs": [],
   "source": [
    "# You can use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.text import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f5ae2",
   "metadata": {
    "id": "4631a076-ff58-4f64-bb16-4a322df5f8e3"
   },
   "source": [
    "## Preprocessing\n",
    "### Load the document (Company policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7870d3",
   "metadata": {
    "id": "6195ebc2-8bb7-4796-9a62-91bd26acbf9d"
   },
   "outputs": [],
   "source": [
    "filename = 'companyPolicies.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    # Read the contents of the file\n",
    "    contents = file.read()\n",
    "    print(len(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a534c6b",
   "metadata": {
    "id": "d5d8ad85-ff84-4903-a694-914db4b00134"
   },
   "source": [
    "### Splitting the document into chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecfca7",
   "metadata": {
    "id": "c2ef7183-d704-47ce-af07-ab605c5d5033"
   },
   "source": [
    "In this step, we split the document into smaller parts, called chunks, using LangChain. This helps manage large documents efficiently.\n",
    "\n",
    "Key Points:\n",
    "- Chunking is part of the Indexing process.\n",
    "\n",
    "- LangChain's CharacterTextSplitter is used to divide the document into smaller sections.\n",
    "\n",
    "- Chunk size: Set to 1000 characters in this project.\n",
    "\n",
    "- Default separator: \\n\\n (double newline).\n",
    "\n",
    "- Custom separator: Can be changed using the separator parameter (e.g., separator=\"\\n\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78ff0e",
   "metadata": {
    "id": "99f7d8ca-f2d8-46fc-a2d8-d3c356c22bac"
   },
   "outputs": [],
   "source": [
    "loader = TextLoader(filename)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736fab7f",
   "metadata": {
    "id": "7ace86dc-a6c3-411e-b470-3899bd49489f"
   },
   "source": [
    "### Embedding and storing\n",
    "In this step, the text chunks are converted into numerical representations using a process called embedding. This helps the computer efficiently recognize and retrieve relevant information later.\n",
    "\n",
    "The embedding process happens during Indexing, ensuring quick and accurate searches within the document. The following code creates an embedding model using Hugging Face and stores the embeddings in ChromaDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4a33a",
   "metadata": {
    "id": "996871e4-0666-4937-88ab-2b4f39e8c627"
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Convert text documents to vectors\n",
    "print(\"Converting text documents to vectors...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': False}  # Reduces memory usage\n",
    ")\n",
    "\n",
    "print(\"Storing vectors in ChromaDB and saving to ./chroma_db.\")\n",
    "docsearch = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings,  # Now explicitly bound to the DB\n",
    "    persist_directory=\"./chroma_db\"  # Allows reloading later\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b87395",
   "metadata": {
    "id": "zwX5VRoFbDs2"
   },
   "outputs": [],
   "source": [
    "# Purpose: Reload existing ChromaDB for queries\n",
    "# When to Run: After restarting your notebook/kernel\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Reinitialize embeddings (MUST match initial config)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': False}\n",
    ")\n",
    "\n",
    "# Load existing ChromaDB\n",
    "docsearch = Chroma(\n",
    "    persist_directory=\"./chroma_db\",  # Load from disk\n",
    "    embedding_function=embeddings   # Required for queries\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = docsearch.as_retriever()\n",
    "print(\"✅ ChromaDB and retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406f2db9",
   "metadata": {
    "id": "0faf57a1-c04c-4194-9863-de7610a57560"
   },
   "source": [
    "Up to this point, you've been performing the `Indexing` task. The next step is the `Retrieval` task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b30b3",
   "metadata": {
    "id": "nkh0uT8kvfBk"
   },
   "source": [
    "###Retrieve (Fetching Relevant Documents)\n",
    "\n",
    "The user’s question is encoded into a vector, which is then used to fetch the most relevant documents from the ChromaDB vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6060f83b",
   "metadata": {
    "id": "SF7lDG2pwQDP"
   },
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(\n",
    "    search_type=\"similarity\",  # Default, good for policy QA\n",
    "    search_kwargs={\n",
    "        \"k\": 1  # Number of documents to retrieve\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"What is the mobile policy?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(\"\\nTop Retrieved Documents:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"{i+1}. {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5866e",
   "metadata": {
    "id": "BmE7ybvTwlP2"
   },
   "source": [
    "✅ This ensures that the retrieval process works before proceeding to LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f540d936",
   "metadata": {
    "id": "1B5mOKHywsTl"
   },
   "source": [
    "###Prompt (Structuring the Input for LLM)\n",
    "The retrieved documents are formatted into a structured prompt that the LLM can use to generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e67654",
   "metadata": {
    "id": "OPRQuWx_w2FU"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "policy_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"Analyze the following policy documents and provide a concise answer to the question.\n",
    "\n",
    "Policy Documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in this format:\n",
    "- Start with \"According to company policy:\" if relevant\n",
    "- Provide a 1-2 sentence summary\n",
    "- List key points as bullet items\n",
    "- Highlight important numbers/dates like [25 feet]\n",
    "- If no policy is found, say \"This policy isn't covered in current documents\"\n",
    "\n",
    "Direct answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e3d07",
   "metadata": {
    "id": "xRRRcRCLxC3f"
   },
   "source": [
    "✅ Ensures the LLM receives structured and useful input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b6a149",
   "metadata": {
    "id": "nHFz1lQxxJP-"
   },
   "source": [
    "###LLM (Generating the Final Answer)\n",
    "We use a foundation model to process the prompt and generate a human-like response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce5414",
   "metadata": {
    "id": "yX5TJn8lxVQn"
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load a small CPU-friendly model\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a local pipeline\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.3,\n",
    "    device=\"cpu\"  # Force CPU usage\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18b358",
   "metadata": {
    "id": "g9cF__ach_Wa"
   },
   "source": [
    "###Chatbot with Memory\n",
    "We'll use LangChain's ConversationBufferMemory. This will allow the bot to remember previous interactions in the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f014a",
   "metadata": {
    "id": "NVMhxbVYdxfm"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "\n",
    "# Initialize components\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "docsearch = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key='answer'\n",
    ")\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": policy_prompt},\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Modified UI Elements with memory testing features\n",
    "input_box = widgets.Text(placeholder='Ask about policies...', layout=widgets.Layout(width='80%'))\n",
    "output_area = widgets.Output(layout=widgets.Layout(max_height='500px', overflow='auto'))\n",
    "submit_btn = widgets.Button(description=\"Ask\", button_style='primary')\n",
    "memory_test_btn = widgets.Button(description=\"Test Memory\", button_style='info')\n",
    "\n",
    "def show_thinking():\n",
    "    \"\"\"Display thinking animation\"\"\"\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        display(HTML(\"\"\"\n",
    "        <div style='text-align:center'>\n",
    "            <div style='border:5px solid #f3f3f3; border-top:5px solid #3498db; border-radius:50%;\n",
    "                      width:30px; height:30px; animation:spin 1s linear infinite; margin:10px auto;'></div>\n",
    "            <p>Searching policies...</p>\n",
    "        </div>\n",
    "        <style>@keyframes spin {0%{transform:rotate(0deg);}100%{transform:rotate(360deg);}}</style>\n",
    "        \"\"\"))\n",
    "\n",
    "def test_memory(b):\n",
    "    \"\"\"Explicitly test memory functionality\"\"\"\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        if not memory.chat_memory.messages:\n",
    "            print(\"🤖 No conversation history yet\")\n",
    "            return\n",
    "\n",
    "        print(\"🧠 Memory Contents:\")\n",
    "        for i, msg in enumerate(memory.chat_memory.messages):\n",
    "            prefix = \"👤 Question\" if i % 2 == 0 else \"🤖 Answer\"\n",
    "            print(f\"{prefix} {i//2 + 1}: {msg.content[:200]}{'...' if len(msg.content) > 200 else ''}\")\n",
    "\n",
    "def on_submit(b):\n",
    "    question = input_box.value.strip()\n",
    "    if not question:\n",
    "        with output_area:\n",
    "            print(\"⚠️ Please enter a question\")\n",
    "        return\n",
    "\n",
    "    show_thinking()\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = qa_chain({\"question\": question})\n",
    "        response = result[\"answer\"]\n",
    "\n",
    "        if \"Answer in this format:\" in response:\n",
    "            response = response.split(\"Direct answer:\")[-1].strip()\n",
    "\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(f\"👤 You: {question}\")\n",
    "            print(f\"🤖 Bot: {response}\")\n",
    "            print(f\"\\n⏱️ Response time: {time.time()-start_time:.1f} seconds\")\n",
    "\n",
    "            # Enhanced memory display\n",
    "            print(\"\\n--- Current Conversation Context ---\")\n",
    "            print(f\"Total exchanges: {len(memory.chat_memory.messages)//2}\")\n",
    "\n",
    "            # Show last 2 exchanges with clear labels\n",
    "            for i, msg in enumerate(memory.chat_memory.messages[-4:]):\n",
    "                if i % 2 == 0:\n",
    "                    print(f\"\\n🗣️ Q{i//2 + 1}: {msg.content[:100]}{'...' if len(msg.content) > 100 else ''}\")\n",
    "                else:\n",
    "                    print(f\"🤖 A{i//2 + 1}: {msg.content[:100]}{'...' if len(msg.content) > 100 else ''}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        with output_area:\n",
    "            print(f\"🚨 Error: {str(e)}\")\n",
    "\n",
    "    input_box.value = \"\"\n",
    "\n",
    "# Display enhanced interface\n",
    "display(widgets.VBox([\n",
    "    widgets.Label(\"💼 Policy Assistant (Memory Test Mode)\", style={'font_weight':'bold'}),\n",
    "    input_box,\n",
    "    widgets.HBox([submit_btn, memory_test_btn]),\n",
    "    output_area\n",
    "]))\n",
    "\n",
    "submit_btn.on_click(on_submit)\n",
    "memory_test_btn.on_click(test_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759185f",
   "metadata": {
    "id": "dcZUSeWiyLPS"
   },
   "source": [
    "###COMPANY POLICY QA AGENT\n",
    "\n",
    "The following AI agent answers HR policy questions by searching company documents. It combines document retrieval with smart reasoning to provide accurate responses, while maintaining conversation history. The interface shows real-time status and prevents hallucinations by falling back to \"Contact HR\" when uncertain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1795be",
   "metadata": {
    "id": "SzkM5dPznD8g"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, initialize_agent\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def policy_agent():\n",
    "    # 1. Optimized Retrieval QA for TinyLlama\n",
    "    qa_prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"Summarize this policy in 100 words or less:\n",
    "\n",
    "        Policy Excerpt: {context}\n",
    "        Question: {question}\n",
    "\n",
    "        Answer MUST BE:\n",
    "        - 2 to 3 sentences maximum\n",
    "\n",
    "        - If unclear, respond \"See HR handbook section [X.Y]\"\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=docsearch.as_retriever(\n",
    "            search_kwargs={\n",
    "                \"k\": 1,  # Only 1 document\n",
    "                \"score_threshold\": 0.8  # High confidence matches only\n",
    "            }\n",
    "        ),\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": qa_prompt,\n",
    "            \"document_variable_name\": \"context\"\n",
    "        },\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # 2. Tools with TinyLlama-optimized limits\n",
    "    def get_policy(query):\n",
    "        try:\n",
    "            result = qa.run(question=query[:50])  # Very short questions\n",
    "            return result[:150]  # Hard token limit for TinyLlama\n",
    "        except:\n",
    "            return \"Contact HR for policy details\"\n",
    "\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"HR_Policy_Lookup\",\n",
    "            func=get_policy,\n",
    "            description=\"ONLY for verbatim policy quotes. Input must be under 50 characters.\"\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"HR_Contact\",\n",
    "            func=lambda _: \"Please contact HR at hr@company.com\",\n",
    "            description=\"Use when policy is not found or question is too complex\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 3. Using CONVERSATIONAL_REACT_DESCRIPTION instead\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "        memory=ConversationBufferMemory(memory_key=\"chat_history\"),\n",
    "        verbose=False,\n",
    "        max_execution_time=5,  # 5s timeout for CPU\n",
    "        max_iterations=1,  # Only 1 reasoning step\n",
    "        handle_parsing_errors=\"Please ask a shorter policy question (max 50 chars)\"\n",
    "    )\n",
    "\n",
    "    print(\"HR Policy Assistant (TinyLlama CPU) ready. Ask short questions or 'quit':\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuestion: \").strip()[:50]  # Hard input limit\n",
    "            if query.lower() in [\"quit\", \"exit\"]:\n",
    "                break\n",
    "\n",
    "            response = agent.run(input=query)\n",
    "            # Clean the output to remove agent thoughts if present\n",
    "            if \"Final Answer:\" in response:\n",
    "                response = response.split(\"Final Answer:\")[-1].strip()\n",
    "            print(f\"Answer: {response[:150]}\")  # Enforce output length\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Please ask a simpler policy question or contact HR\")\n",
    "\n",
    "policy_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
